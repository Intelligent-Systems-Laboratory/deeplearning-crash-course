{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiiKgI6Do5n"
   },
   "source": [
    "# End-To-End Neural Network in Pytorch\n",
    "- In this notebook, we explore the usage of pytorch framework in the whole spectrum\n",
    "- We also to see the different perspectives on training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: drive/My Drive/mnist\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127cbde80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADxRJREFUeJzt3X2MVfWdx/HPdykYEYgPPOxoQUqjmy0mO+igm/iQWY0GNhioCaZENxDrgljjNm50iRGRrDVoaF3/gYRGUiCtpQEfkOjSZrJilY1hJLXyVGpwpCwTZlEThphIgO/+MYfNgHN/5859Onf4vl8Jufee7/3d8+Xqh3PuPefcn7m7AMTzV0U3AKAYhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFDfauTKzIzTCYE6c3cr53lVbfnNbIaZ/cnMPjGzJdW8FoDGskrP7TezYZIOSLpT0mFJOyXNc/e9iTFs+YE6a8SW/0ZJn7j7QXc/KenXkmZX8XoAGqia8F8l6S/9Hh/Olp3DzBaaWaeZdVaxLgA1Vs0XfgPtWnxjt97d10haI7HbDzSTarb8hyVN7Pf425KOVNcOgEapJvw7JV1jZt8xsxGSfiBpS23aAlBvFe/2u/spM3tE0jZJwyStdfc9NesMQF1VfKivopXxmR+ou4ac5ANg6CL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKiGTtGNoWfmzJnJ+qxZs5L1hx56qJbtnKOzMz0D3N13312y1tPTU+t2hhy2/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QVFWz9JpZl6ReSaclnXL3tpznM0tvg40ePTpZX7ZsWbK+aNGiZH3kyJHJeup4em9vb3Ls8OHDk/WJEycm619//XXJ2l133ZUc+/777yfrzazcWXprcZLPP7j7sRq8DoAGYrcfCKra8Luk35rZh2a2sBYNAWiManf7b3b3I2Y2XtLvzGy/u7/b/wnZPwr8wwA0maq2/O5+JLvtkfSapBsHeM4ad2/L+zIQQGNVHH4zu8TMRp+9L+kuSbtr1RiA+qpmt3+CpNfM7Ozr/Mrd/7MmXQGou4rD7+4HJf1dDXtpaqnj5StXrkyOvfrqq5P1efPmJetffvllsj5lypSStY0bNybHTps2LVnP+7t1dHQk659++mnJWt7fa8SIEcn6ggULkvVnn322ZK29vT05digf5y8Xh/qAoAg/EBThB4Ii/EBQhB8IivADQVV1Se+gVzaEL+lNHY7bsGFDcuzBgweT9ba29MmPra2tyfrzzz9fsrZt27bk2E2bNiXr+/fvT9ZPnTqVrBfp9OnTJWsfffRRcuz1119f63YaptxLetnyA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQTNFdphkzZlQ8dseOHcn68ePHk/UxY8Yk648//njJ2nvvvZcc28ymTp2arC9durTi1965c2fFYy8UbPmBoAg/EBThB4Ii/EBQhB8IivADQRF+ICiO82eGDRuWrF988cUla9ncBRXX82zdurWq8c3qoosuStZT5y9I0ty5c5P11PTgL730UnJsBGz5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3OP8ZrZW0ixJPe5+XbbsckkbJU2W1CXpXndPz7fc5MaNG5es33PPPSVreXMfvPXWWxX1dCGYNGlSyVre9fj3339/sp73vt9xxx0la3v37k2OjaCcLf8vJJ3/SxZLJHW4+zWSOrLHAIaQ3PC7+7uSvjhv8WxJ67L76yTNqXFfAOqs0s/8E9y9W5Ky2/G1awlAI9T93H4zWyhpYb3XA2BwKt3yHzWzFknKbkteQeHua9y9zd3Ts1ECaKhKw79F0vzs/nxJb9SmHQCNkht+M3tF0n9L+hszO2xmP5S0QtKdZvZnSXdmjwEMIbmf+d291MT0pQ+i4hx79uwpuoW6aW1tTdbffPPNkrWWlpaq1t3Z2Zmsjx9f+ntojvNzhh8QFuEHgiL8QFCEHwiK8ANBEX4gKH66O7gpU6Yk648++miyPm9eqSPBfa644opB91Sutrb0SaPbtm0rWdu+fXty7Pz585P17u7uZH0oYMsPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0FxnL8Bpk+fnqzv3r27qtdPXbr61FNPJcfm/Tz2mDFjkvW86cdTP6+9adOm5NjPPvssWc87zt/e3l6ydvvttyfH5k2Lnve+vv3228l6M2DLDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBWd40xzVdmVnjVlZj69evL1m77777qnrtVatWJes33XRTsn7DDTdUtf6Ur776Kll/4oknkvXVq1fXsp1BSR3nX758eXLsLbfcUtW6r7zyymT96NGjVb1+irunT77IsOUHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaByr+c3s7WSZknqcffrsmXPSPpnSf+bPe1Jd3+rXk02g9S15b29vcmxo0aNStYXL16crFdzzXye119/PVl/7rnnkvVdu3ZVvO56e+edd0rW5syZU/FYSZo6dWqy3sjzZypVzpb/F5JmDLD8RXdvzf5c0MEHLkS54Xf3dyV90YBeADRQNZ/5HzGzP5rZWjO7rGYdAWiISsO/WtJ3JbVK6pb001JPNLOFZtZpZp0VrgtAHVQUfnc/6u6n3f2MpJ9LujHx3DXu3ubu6V9bBNBQFYXfzFr6Pfy+pOp+fhZAw5VzqO8VSe2SxprZYUnLJLWbWaskl9QlaVEdewRQB7nhd/eBJmB/uQ69NLWlS5eWrB06dCg5ttpr2g8cOJCsd3V1laxt3rw5OXbjxo3Jet45DEPVqVOnkvWTJ082qJPicIYfEBThB4Ii/EBQhB8IivADQRF+ICim6K6BvKmkq/XYY48l60NhOuhms2zZsmR92rRpDeqkOGz5gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAojvM3gc7O9C+ccRy/Mqlj+YsWVfcTFEuWLEnWjx07VtXrNwJbfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IiuP8DZA3xfb06dMb1MnQcttttyXry5cvT9bb29tL1s6cOZMcmzd1+cqVK5P1oYAtPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ElXuc38wmSlov6a8lnZG0xt1fMrPLJW2UNFlSl6R73f3L+rU6dLl7VeNnzpyZrDfz9f6TJk0qWXvwwQeTYx9++OFk/dJLL03WU9Nsv/DCC8mxq1atStYvBOVs+U9J+ld3/1tJfy/pR2b2PUlLJHW4+zWSOrLHAIaI3PC7e7e778ru90raJ+kqSbMlrcuetk7SnHo1CaD2BvWZ38wmS5om6QNJE9y9W+r7B0LS+Fo3B6B+yj6338xGSdos6cfufjzvfPV+4xZKWlhZewDqpawtv5kNV1/wf+nur2aLj5pZS1ZvkdQz0Fh3X+Pube7eVouGAdRGbvitbxP/sqR97v6zfqUtkuZn9+dLeqP27QGol3J2+2+W9E+SPjazP2TLnpS0QtJvzOyHkg5JmlufFpvf559/nqyfOHEiWR81alSy/uKLLybr1157bcnavn37kmO7urqS9cmTJyfrt956a7K+YMGCkrWWlpbk2Dx5U6M/8MADJWvbt2+vat0Xgtzwu/t7kkp9wL+jtu0AaBTO8AOCIvxAUIQfCIrwA0ERfiAowg8EZdVebjqolZk1bmVNZM6c9DVPGzZsSNZHjhyZrFfz3zBvKumxY8cm63mnead66+joSI5dsWJFsr5///5kvbu7O1m/ULl7Wefes+UHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaA4zt8Exo0bl6w//fTTyfrixYtr2c45Ojs7k/W86+JTPyu+Y8eO5NjUT2+jNI7zA0gi/EBQhB8IivADQRF+ICjCDwRF+IGgOM4PXGA4zg8gifADQRF+ICjCDwRF+IGgCD8QFOEHgsoNv5lNNLP/MrN9ZrbHzP4lW/6Mmf2Pmf0h+/OP9W8XQK3knuRjZi2SWtx9l5mNlvShpDmS7pV0wt1Xlr0yTvIB6q7ck3y+VcYLdUvqzu73mtk+SVdV1x6Aog3qM7+ZTZY0TdIH2aJHzOyPZrbWzC4rMWahmXWaWfr3oAA0VNnn9pvZKEnbJf3E3V81swmSjklySf+uvo8GD+S8Brv9QJ2Vu9tfVvjNbLikrZK2ufvPBqhPlrTV3a/LeR3CD9RZzS7ssb5pWF+WtK9/8LMvAs/6vqTdg20SQHHK+bb/Fkm/l/SxpDPZ4iclzZPUqr7d/i5Ji7IvB1OvxZYfqLOa7vbXCuEH6o/r+QEkEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4LK/QHPGjsm6bN+j8dmy5pRs/bWrH1J9FapWvZ2dblPbOj1/N9YuVmnu7cV1kBCs/bWrH1J9Faponpjtx8IivADQRUd/jUFrz+lWXtr1r4keqtUIb0V+pkfQHGK3vIDKEgh4TezGWb2JzP7xMyWFNFDKWbWZWYfZzMPFzrFWDYNWo+Z7e637HIz+52Z/Tm7HXCatIJ6a4qZmxMzSxf63jXbjNcN3+03s2GSDki6U9JhSTslzXP3vQ1tpAQz65LU5u6FHxM2s9sknZC0/uxsSGb2gqQv3H1F9g/nZe7+b03S2zMa5MzNdeqt1MzSC1Tge1fLGa9roYgt/42SPnH3g+5+UtKvJc0uoI+m5+7vSvrivMWzJa3L7q9T3/88DVeit6bg7t3uviu73yvp7MzShb53ib4KUUT4r5L0l36PD6u5pvx2Sb81sw/NbGHRzQxgwtmZkbLb8QX3c77cmZsb6byZpZvmvatkxutaKyL8A80m0kyHHG529+slzZT0o2z3FuVZLem76pvGrVvST4tsJptZerOkH7v78SJ76W+Avgp534oI/2FJE/s9/rakIwX0MSB3P5Ld9kh6TX0fU5rJ0bOTpGa3PQX38//c/ai7n3b3M5J+rgLfu2xm6c2Sfunur2aLC3/vBuqrqPetiPDvlHSNmX3HzEZI+oGkLQX08Q1mdkn2RYzM7BJJd6n5Zh/eIml+dn++pDcK7OUczTJzc6mZpVXwe9dsM14XcpJPdijjPyQNk7TW3X/S8CYGYGZT1Le1l/quePxVkb2Z2SuS2tV31ddRScskvS7pN5ImSTokaa67N/yLtxK9tWuQMzfXqbdSM0t/oALfu1rOeF2TfjjDD4iJM/yAoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwT1f6ldffCJriIeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "            In the constructor we instantiate three nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            Note: we just define components. we have not yet connected them. \n",
    "            If you think of it like a graph, we are only declaring nodes, we have not declared edges\n",
    "\n",
    "            Input:\n",
    "            D_in: a scalar value that defines the no. of neurons on layer 0 (a.k.a. input layer)\n",
    "            H1: number of neurons in layer 1\n",
    "            H2: number of neurons in layer 2\n",
    "            D_out: number of neurons in layer 3\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__() #Call the parent class\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in,H1) # Linear => Linear Transformation. In equation, this is the same as Wx + b.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H1,H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H2,D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Softmax is simply sigmoid for multiple classes. That is, if we only have two classes, softmax == sigmoid.\n",
    "        #Note: We need softmax/sigmoid at the end if it's a classification task. If not, we skip this step.\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"\"\"\n",
    "          This is where you define your forward propagation.\n",
    "          In other words, this is where we combine and connect all the components we defined above.\n",
    "\n",
    "          Input:\n",
    "          x0 = the actual image in batch\n",
    "\n",
    "        \"\"\"\n",
    "        # Method View = reshape the tensor into a different dimension.\n",
    "        # x0.shape[0] = we get the first dim of the image shape. Recall: This would be batch_size\n",
    "        # -1 = The -1 as the second argument means compute the remaining and that would be my second dimension.\n",
    "        # Thus, the whole line below means, I want to reshape this batch of images by doing the ff:\n",
    "        # The first dim of this new 'view' would be based on the first dim of this image. In this case, I would get 64\n",
    "        # The second dim of this new 'view' would be computed based on the remaining pixels left. Thus, it would be 28*28=784.\n",
    "        # Thus, The new view would be (64,784)\n",
    "        x0 = x0.view(x0.shape[0],-1)\n",
    "        x1 = self.relu1(self.linear1(x0))\n",
    "        x2 = self.relu2(self.linear2(x1))\n",
    "        x3 = self.softmax(self.linear3(x2)) \n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "model = MLP(D_in,H1,H2,D_out)\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v2.1\n",
    "- Ealier, we developed the full training and test routine.\n",
    "- However, it's too lengthy and wordy. We can refactor it so that we can improve reusability and readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/9\n",
      "Epoch: 0 train Loss: 18.9371 Acc: 91.4633\n",
      "Epoch: 0 val Loss: 18.2302 Acc: 91.7100\n",
      "epoch 1/9\n",
      "Epoch: 1 train Loss: 18.7291 Acc: 91.5850\n",
      "Epoch: 1 val Loss: 18.0234 Acc: 91.8700\n",
      "epoch 2/9\n",
      "Epoch: 2 train Loss: 18.5378 Acc: 91.6700\n",
      "Epoch: 2 val Loss: 17.8852 Acc: 91.8500\n",
      "epoch 3/9\n",
      "Epoch: 3 train Loss: 18.3560 Acc: 91.7050\n",
      "Epoch: 3 val Loss: 17.6857 Acc: 91.9900\n",
      "epoch 4/9\n",
      "Epoch: 4 train Loss: 18.1639 Acc: 91.8467\n",
      "Epoch: 4 val Loss: 17.5239 Acc: 92.0200\n",
      "epoch 5/9\n",
      "Epoch: 5 train Loss: 17.9892 Acc: 91.9250\n",
      "Epoch: 5 val Loss: 17.3920 Acc: 92.0800\n",
      "epoch 6/9\n",
      "Epoch: 6 train Loss: 17.8282 Acc: 91.9533\n",
      "Epoch: 6 val Loss: 17.2497 Acc: 92.1900\n",
      "epoch 7/9\n",
      "Epoch: 7 train Loss: 17.6609 Acc: 92.0117\n",
      "Epoch: 7 val Loss: 17.0760 Acc: 92.2200\n",
      "epoch 8/9\n",
      "Epoch: 8 train Loss: 17.4938 Acc: 92.0967\n",
      "Epoch: 8 val Loss: 16.9600 Acc: 92.3800\n",
      "epoch 9/9\n",
      "Epoch: 9 train Loss: 17.3308 Acc: 92.2000\n",
      "Epoch: 9 val Loss: 16.8140 Acc: 92.3600\n"
     ]
    }
   ],
   "source": [
    "dataloader = {}\n",
    "dataloader['train'] = trainDataLoader\n",
    "dataloader['val'] = testDataLoader\n",
    "for i in range(epochs):\n",
    "  \n",
    "    print(\"epoch {}/{}\".format(i,epochs-1))\n",
    "    for phase in [\"train\",\"val\"]:\n",
    "        # Training phase\n",
    "        running_loss = 0\n",
    "        correct_counter = 0\n",
    "        all_counter = 0\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        for images, labels in dataloader[phase]:\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # Forward propagation\n",
    "                y_pred = model(images)\n",
    "\n",
    "                # Get highest value at column\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "\n",
    "                #compute the loss\n",
    "                loss = criterion(y_pred,labels)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # Calculating gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct_counter += torch.sum(preds == labels)\n",
    "\n",
    "        ave_loss = running_loss/len(dataloader[phase])\n",
    "        all_counter += len(dataloader[phase].dataset)  \n",
    "        accuracy = (correct_counter.double() / all_counter).item()\n",
    "\n",
    "        print(\"Epoch: {} {} Loss: {:.4f} Acc: {:.4f}\".format(i, phase, ave_loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNv0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
