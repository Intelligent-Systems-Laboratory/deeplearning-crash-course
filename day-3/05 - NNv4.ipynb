{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiiKgI6Do5n"
   },
   "source": [
    "# End-To-End Neural Network in Pytorch\n",
    "- In this notebook, we explore the usage of pytorch framework in the whole spectrum\n",
    "- We also to see the different perspectives on training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: drive/My Drive/mnist\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fcf685aa240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANHElEQVR4nO3dYahc9ZnH8d8vd1sQ2xeJRk1ttN0iibKwVi+hkBK6lNasb5JwcWmQ4IJ4+6KuUQq70X1RQQTZ3W7ZV423VJpKtRRiqC/C2ksopr6JxpDVmGurK9kmzSXRzYta8qKrefrinnRv4p3/mcw5M2fi8/3AZWbOM2fmyejvnnPnnP/5OyIE4ONvWdcNABgNwg4kQdiBJAg7kARhB5L4i1G+mW2++geGLCK81PJGW3bbG23/2vbbtnc0eS0Aw+VBj7PbnpD0G0lfk3RC0iuStkbE0cI6bNmBIRvGln2dpLcj4p2I+KOkn0ra1OD1AAxRk7BfL+n4oscnqmUXsD1t+6Dtgw3eC0BDTb6gW2pX4SO76RExI2lGYjce6FKTLfsJSasXPf6spJPN2gEwLE3C/oqkm2x/3vYnJX1D0vPttAWgbQPvxkfEB7bvl/SCpAlJT0XEG611BqBVAx96G+jN+JsdGLqhnFQD4PJB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUhi4PnZJcn2MUnvS/pQ0gcRMdlGUwDa1yjslb+JiPdaeB0AQ8RuPJBE07CHpF/YftX29FJPsD1t+6Dtgw3fC0ADjojBV7Y/ExEnbV8jaVbSP0TE/sLzB38zAH2JCC+1vNGWPSJOVrenJe2RtK7J6wEYnoHDbvtK258+f1/S1yUdaasxAO1q8m38tZL22D7/Os9ExH+20hUuycaNG3vWHn744UavPTc3V6y/8MILxfqePXsavT/aM3DYI+IdSX/dYi8AhohDb0AShB1IgrADSRB2IAnCDiTRxkAYNLRy5cpive7w2fbt23vW6s6QrA6d9rR+/fpi/Y477ijWS/+2mZmZ4rpoF1t2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii0ZVqLvnNkl6pZsOGDcX6zp07i/U1a9YU62fPnu1ZqxtievPNNxfrdUNcJyfLFxQu9T4xMVFcF4MZypVqAFw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCY6zt6BuPPrevXuL9dtuu61Yr/tvdNddd/WsDftSzo899lixXhqLPzs7W1x327Ztxfp77zGf6FI4zg4kR9iBJAg7kARhB5Ig7EAShB1IgrADSXDd+D6VjqW/+OKLxXXrxqM/88wzxfqTTz5ZrL/00kvFehN15xBMTU0V66Xr0tddc/6GG24o1jnOfmlqt+y2n7J92vaRRctW2J61/VZ1u3y4bQJoqp/d+B9J2njRsh2S9kXETZL2VY8BjLHasEfEfklnLlq8SdKu6v4uSZtb7gtAywb9m/3aiJiXpIiYt31NryfanpY0PeD7AGjJ0L+gi4gZSTPSx3cgDHA5GPTQ2ynbqySpuj3dXksAhmHQsD8v6Z7q/j2Sft5OOwCGpXY33vazkr4i6WrbJyR9R9ITkn5m+15Jv5XUe0D1x8SWLVt61uqOo9eNR3/ooYeK9S6PJ9fNDd/0347RqQ17RGztUfpqy70AGCJOlwWSIOxAEoQdSIKwA0kQdiAJhrj26b777utZKw3jlKSZmZlivctDazfeeGOxXjcMte7fPqx1cenYsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEhxnb0HXwzinp3tf9as0NFeqny76qquuKtbr/u1dfzb4f2zZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJj/I46OU8I8zTTz/ds3b33XcX1637jOvGdTdZv8v3rlv/+PHjxXUnJyeLdaZsXlpELPkfhS07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBePY+bdu2rWftiiuuKK67efPmRu/d5FyIo0ePFuvLlpV/3587d65Yv+WWW4r1Uu+MdR+t2i277adsn7Z9ZNGyR23/zvbh6ufO4bYJoKl+duN/JGnjEsu/FxG3Vj97220LQNtqwx4R+yWdGUEvAIaoyRd099t+rdrNX97rSbanbR+0fbDBewFoaNCwf1/SFyTdKmle0nd7PTEiZiJiMiLKoxoADNVAYY+IUxHxYUSck/QDSevabQtA2wYKu+1Vix5ukXSk13MBjIfa8ey2n5X0FUlXSzol6TvV41slhaRjkr4ZEfO1b3YZj2dvYu3atcX61NRUsV53nL40rrt0fkDduv3YuXNnsd5kXvu68eyHDh0q1rPqNZ699qSaiNi6xOIfNu4IwEhxuiyQBGEHkiDsQBKEHUiCsANJMMR1BN58881i/fHHH29UH2cMYx0fbNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAmmbEYjt99+e7H+8ssv96w1nS56YmKiWM+KKZuB5Ag7kARhB5Ig7EAShB1IgrADSRB2IAnGs2OompzH8dxzz7XYCdiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGfHUJXGrNeNZ286nTQuVLtlt73a9i9tz9l+w/b2avkK27O236pulw+/XQCD6mc3/gNJ346ImyV9SdK3bN8iaYekfRFxk6R91WMAY6o27BExHxGHqvvvS5qTdL2kTZJ2VU/bJWnzsJoE0Nwl/c1u+3OSvijpgKRrI2JeWviFYPuaHutMS5pu1iaApvoOu+1PSdot6cGI+H3dlyvnRcSMpJnqNbjgJNCRvg692f6EFoL+k4g4PxTplO1VVX2VpNPDaRFAG2ovJe2FTfguSWci4sFFy/9V0v9GxBO2d0haERH/WPNabNk/ZuouJX3gwIGetbq9w3fffbdYv+6664r1rHpdSrqf3fj1krZJet324WrZI5KekPQz2/dK+q2ku9poFMBw1IY9Il6S1OtX8FfbbQfAsHC6LJAEYQeSIOxAEoQdSIKwA0kwxBVDVTqWvmxZeVuzcuXKYn3Dhg3F+v79+4v1bNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASHGdHI3Nzc8X6nj17eta2bNlSXLfuWgtr164t1jnOfiG27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBMfZ0cjZs2eL9dnZ2Z61qamp4rrnzp0r1vudlQgL2LIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBK1x9ltr5b0Y0nXSTonaSYi/sP2o5Luk3R+Eu1HImLvsBrF5Wn37t09aw888EBx3TVr1jSq40L9nFTzgaRvR8Qh25+W9Krt82dKfC8i/m147QFoSz/zs89Lmq/uv297TtL1w24MQLsu6W9225+T9EVJB6pF99t+zfZTtpf3WGfa9kHbBxt1CqCRvsNu+1OSdkt6MCJ+L+n7kr4g6VYtbPm/u9R6ETETEZMRMdlCvwAG1FfYbX9CC0H/SUQ8J0kRcSoiPoyIc5J+IGnd8NoE0FRt2L0wtOiHkuYi4t8XLV+16GlbJB1pvz0AbXHd5Xptf1nSryS9roVDb5L0iKStWtiFD0nHJH2z+jKv9FrlNwPQWEQsOfa3NuxtIuzA8PUKO2fQAUkQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkhj1lM3vSfqfRY+vrpaNo3HtbVz7kuhtUG32dmOvwkjHs3/kze2D43ptunHtbVz7kuhtUKPqjd14IAnCDiTRddhnOn7/knHtbVz7kuhtUCPprdO/2QGMTtdbdgAjQtiBJDoJu+2Ntn9t+23bO7rooRfbx2y/bvtw1/PTVXPonbZ9ZNGyFbZnbb9V3S45x15HvT1q+3fVZ3fY9p0d9bba9i9tz9l+w/b2anmnn12hr5F8biP/m932hKTfSPqapBOSXpG0NSKOjrSRHmwfkzQZEZ2fgGF7g6Q/SPpxRPxVtexfJJ2JiCeqX5TLI+KfxqS3RyX9oetpvKvZilYtnmZc0mZJf68OP7tCX3+nEXxuXWzZ10l6OyLeiYg/SvqppE0d9DH2ImK/pDMXLd4kaVd1f5cW/mcZuR69jYWImI+IQ9X99yWdn2a808+u0NdIdBH26yUdX/T4hMZrvveQ9Avbr9qe7rqZJVx7fpqt6vaajvu5WO003qN00TTjY/PZDTL9eVNdhH2pqWnG6fjf+oi4TdLfSvpWtbuK/vQ1jfeoLDHN+FgYdPrzproI+wlJqxc9/qykkx30saSIOFndnpa0R+M3FfWp8zPoVrenO+7nz8ZpGu+lphnXGHx2XU5/3kXYX5F0k+3P2/6kpG9Ier6DPj7C9pXVFyeyfaWkr2v8pqJ+XtI91f17JP28w14uMC7TePeaZlwdf3adT38eESP/kXSnFr6R/29J/9xFDz36+ktJ/1X9vNF1b5Ke1cJu3f9pYY/oXklXSdon6a3qdsUY9fa0Fqb2fk0LwVrVUW9f1sKfhq9JOlz93Nn1Z1foaySfG6fLAklwBh2QBGEHkiDsQBKEHUiCsANJEHYgCcIOJPEn07Q5uJgx4uQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "            In the constructor we instantiate three nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            Note: we just define components. we have not yet connected them. \n",
    "            If you think of it like a graph, we are only declaring nodes, we have not declared edges\n",
    "\n",
    "            Input:\n",
    "            D_in: a scalar value that defines the no. of neurons on layer 0 (a.k.a. input layer)\n",
    "            H1: number of neurons in layer 1\n",
    "            H2: number of neurons in layer 2\n",
    "            D_out: number of neurons in layer 3\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__() #Call the parent class\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in,H1) # Linear => Linear Transformation. In equation, this is the same as Wx + b.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H1,H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H2,D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Softmax is simply sigmoid for multiple classes. That is, if we only have two classes, softmax == sigmoid.\n",
    "        #Note: We need softmax/sigmoid at the end if it's a classification task. If not, we skip this step.\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"\"\"\n",
    "          This is where you define your forward propagation.\n",
    "          In other words, this is where we combine and connect all the components we defined above.\n",
    "\n",
    "          Input:\n",
    "          x0 = the actual image in batch\n",
    "\n",
    "        \"\"\"\n",
    "        # Method View = reshape the tensor into a different dimension.\n",
    "        # x0.shape[0] = we get the first dim of the image shape. Recall: This would be batch_size\n",
    "        # -1 = The -1 as the second argument means compute the remaining and that would be my second dimension.\n",
    "        # Thus, the whole line below means, I want to reshape this batch of images by doing the ff:\n",
    "        # The first dim of this new 'view' would be based on the first dim of this image. In this case, I would get 64\n",
    "        # The second dim of this new 'view' would be computed based on the remaining pixels left. Thus, it would be 28*28=784.\n",
    "        # Thus, The new view would be (64,784)\n",
    "        x0 = x0.view(x0.shape[0],-1)\n",
    "        x1 = self.relu1(self.linear1(x0))\n",
    "        x2 = self.relu2(self.linear2(x1))\n",
    "        x3 = self.softmax(self.linear3(x2)) \n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "model = MLP(D_in,H1,H2,D_out)\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tenorboard as Visualizer Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mnist_run_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v4\n",
    "- At this point we are able to think of reusability and abstractions in mind\n",
    "- Lastly, if we have very expensive training (> 1 day), we want to add a plot so that we can easily see our progress\n",
    "- For this tutorial, we would utilize one of the famous plotting tools `Visdom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    writer.add_scalar('training loss', avg_loss, epoch * len(trainDataLoader))\n",
    "    writer.add_scalar('training accuracy', avg_accuracy, epoch * len(trainDataLoader))\n",
    "    \n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    writer.add_scalar('validation loss', avg_loss, epoch * len(testDataLoader))\n",
    "    writer.add_scalar('validation accuracy', avg_accuracy, epoch * len(testDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 2.1273 Acc: 52.0083\n",
      "Epoch: 1 Val Loss: 2.1230 Acc: 52.6300\n",
      "Epoch: 2 train Loss: 1.7639 Acc: 60.5883\n",
      "Epoch: 2 Val Loss: 1.7521 Acc: 61.3100\n",
      "Epoch: 3 train Loss: 1.2690 Acc: 71.6333\n",
      "Epoch: 3 Val Loss: 1.2523 Acc: 72.4700\n",
      "Epoch: 4 train Loss: 0.9288 Acc: 77.7183\n",
      "Epoch: 4 Val Loss: 0.9103 Acc: 78.3800\n"
     ]
    }
   ],
   "source": [
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNv0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
