{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiiKgI6Do5n"
   },
   "source": [
    "# End-To-End Neural Network in Pytorch\n",
    "- In this notebook, we explore the usage of pytorch framework in the whole spectrum\n",
    "- We also to see the different perspectives on training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: drive/My Drive/mnist\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127cbde80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "            In the constructor we instantiate three nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            Note: we just define components. we have not yet connected them. \n",
    "            If you think of it like a graph, we are only declaring nodes, we have not declared edges\n",
    "\n",
    "            Input:\n",
    "            D_in: a scalar value that defines the no. of neurons on layer 0 (a.k.a. input layer)\n",
    "            H1: number of neurons in layer 1\n",
    "            H2: number of neurons in layer 2\n",
    "            D_out: number of neurons in layer 3\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__() #Call the parent class\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in,H1) # Linear => Linear Transformation. In equation, this is the same as Wx + b.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H1,H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H2,D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Softmax is simply sigmoid for multiple classes. That is, if we only have two classes, softmax == sigmoid.\n",
    "        #Note: We need softmax/sigmoid at the end if it's a classification task. If not, we skip this step.\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"\"\"\n",
    "          This is where you define your forward propagation.\n",
    "          In other words, this is where we combine and connect all the components we defined above.\n",
    "\n",
    "          Input:\n",
    "          x0 = the actual image in batch\n",
    "\n",
    "        \"\"\"\n",
    "        # Method View = reshape the tensor into a different dimension.\n",
    "        # x0.shape[0] = we get the first dim of the image shape. Recall: This would be batch_size\n",
    "        # -1 = The -1 as the second argument means compute the remaining and that would be my second dimension.\n",
    "        # Thus, the whole line below means, I want to reshape this batch of images by doing the ff:\n",
    "        # The first dim of this new 'view' would be based on the first dim of this image. In this case, I would get 64\n",
    "        # The second dim of this new 'view' would be computed based on the remaining pixels left. Thus, it would be 28*28=784.\n",
    "        # Thus, The new view would be (64,784)\n",
    "        x0 = x0.view(x0.shape[0],-1)\n",
    "        x1 = self.relu1(self.linear1(x0))\n",
    "        x2 = self.relu2(self.linear2(x1))\n",
    "        x3 = self.softmax(self.linear3(x2)) \n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "model = MLP(D_in,H1,H2,D_out)\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBidRzyfFKGx"
   },
   "source": [
    "# Model Training and Evaluation Framework v1\n",
    "- We train the model using familiar python syntaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "YQqqv665g-HX",
    "outputId": "aaeae8e8-8299-4038-bf62-c8d899ed7204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 28.615031921533124 \n",
      "Epoch: 1 Training loss: 27.306841079105954 \n",
      "Epoch: 2 Training loss: 26.246000328551986 \n",
      "Epoch: 3 Training loss: 25.351701895057012 \n",
      "Epoch: 4 Training loss: 24.59066733368424 \n",
      "Epoch: 5 Training loss: 23.9370984349932 \n",
      "Epoch: 6 Training loss: 23.371875348121627 \n",
      "Epoch: 7 Training loss: 22.8667521517414 \n",
      "Epoch: 8 Training loss: 22.40548612161486 \n",
      "Epoch: 9 Training loss: 21.998763328930462 \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainDataLoader:\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "          # Forward propagation\n",
    "          y_pred = model(images)\n",
    "\n",
    "          #compute the loss\n",
    "          loss = criterion(y_pred,labels)\n",
    "\n",
    "          # Calculating gradients\n",
    "          loss.backward()\n",
    "\n",
    "          # Update parameters\n",
    "          optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    else:\n",
    "        print(\"Epoch: {} Training loss: {} \".format(i, running_loss/len(trainDataLoader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sbr0bz3uo8Ou"
   },
   "source": [
    "## Visualize the Test Set\n",
    "- In this case, we try to get familiarize ourselves on how to use model for predicting 'out-of-training' samples and on how to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "-Sr5wNdnitqP",
    "outputId": "47841739-04fd-4a5a-bd35-8f705a732a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 8\n",
      "True Label: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12862cef0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testIter = iter(testDataLoader) #Convert Loader into built-in Python iterable\n",
    "\n",
    "test_images, test_labels = testIter.next() #Get the next element. Note: each element is has a dim of (batch_size,no_channels,width,height)\n",
    "\n",
    "# As I want to perform inference, disable gradient computation\n",
    "with torch.set_grad_enabled(False):  \n",
    "    y = model(test_images[0]) #Perform computation. Based on my model architecture, my expected output is 1,10\n",
    "    _, y_result = torch.max(y, 1) #Get the maximum along rows.\n",
    "\n",
    "print(\"Predicted digit: {}\".format(y_result[0]))\n",
    "print(\"True Label: {}\".format(test_labels[0]))\n",
    "plt.imshow(test_images[0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6OOhN5queGU"
   },
   "source": [
    "## Evaluate our Model\n",
    "- We now use the whole testset to understand how well is our model.\n",
    "- For this one, we use accuracy as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JW8gpFCxpJEb",
    "outputId": "86a577ed-943b-40f8-d925-0ce8961202d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.6300\n"
     ]
    }
   ],
   "source": [
    "correct_counter = 0\n",
    "all_counter = 0\n",
    "model.eval()\n",
    "for test_images, test_labels in testDataLoader:\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y = model(test_images)\n",
    "\n",
    "        # Get highest value at column\n",
    "        _, preds = torch.max(y, 1)\n",
    "\n",
    "    correct_counter += torch.sum(preds == test_labels)\n",
    "    \n",
    "\n",
    "all_counter += len(testDataLoader.dataset)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(correct_counter.double() / all_counter * 100))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNv0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
