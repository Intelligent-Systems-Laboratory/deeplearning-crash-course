{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiiKgI6Do5n"
   },
   "source": [
    "# End-To-End Neural Network in Pytorch\n",
    "- In this notebook, we explore the usage of pytorch framework in the whole spectrum\n",
    "- We also to see the different perspectives on training models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: drive/My Drive/mnist\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x127cbde80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "            In the constructor we instantiate three nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            Note: we just define components. we have not yet connected them. \n",
    "            If you think of it like a graph, we are only declaring nodes, we have not declared edges\n",
    "\n",
    "            Input:\n",
    "            D_in: a scalar value that defines the no. of neurons on layer 0 (a.k.a. input layer)\n",
    "            H1: number of neurons in layer 1\n",
    "            H2: number of neurons in layer 2\n",
    "            D_out: number of neurons in layer 3\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__() #Call the parent class\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in,H1) # Linear => Linear Transformation. In equation, this is the same as Wx + b.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H1,H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H2,D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Softmax is simply sigmoid for multiple classes. That is, if we only have two classes, softmax == sigmoid.\n",
    "        #Note: We need softmax/sigmoid at the end if it's a classification task. If not, we skip this step.\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"\"\"\n",
    "          This is where you define your forward propagation.\n",
    "          In other words, this is where we combine and connect all the components we defined above.\n",
    "\n",
    "          Input:\n",
    "          x0 = the actual image in batch\n",
    "\n",
    "        \"\"\"\n",
    "        # Method View = reshape the tensor into a different dimension.\n",
    "        # x0.shape[0] = we get the first dim of the image shape. Recall: This would be batch_size\n",
    "        # -1 = The -1 as the second argument means compute the remaining and that would be my second dimension.\n",
    "        # Thus, the whole line below means, I want to reshape this batch of images by doing the ff:\n",
    "        # The first dim of this new 'view' would be based on the first dim of this image. In this case, I would get 64\n",
    "        # The second dim of this new 'view' would be computed based on the remaining pixels left. Thus, it would be 28*28=784.\n",
    "        # Thus, The new view would be (64,784)\n",
    "        x0 = x0.view(x0.shape[0],-1)\n",
    "        x1 = self.relu1(self.linear1(x0))\n",
    "        x2 = self.relu2(self.linear2(x1))\n",
    "        x3 = self.softmax(self.linear3(x2)) \n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "model = MLP(D_in,H1,H2,D_out)\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBidRzyfFKGx"
   },
   "source": [
    "# Model Training and Evaluation Framework v1\n",
    "- We train the model using familiar python syntaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "YQqqv665g-HX",
    "outputId": "aaeae8e8-8299-4038-bf62-c8d899ed7204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 28.615031921533124 \n",
      "Epoch: 1 Training loss: 27.306841079105954 \n",
      "Epoch: 2 Training loss: 26.246000328551986 \n",
      "Epoch: 3 Training loss: 25.351701895057012 \n",
      "Epoch: 4 Training loss: 24.59066733368424 \n",
      "Epoch: 5 Training loss: 23.9370984349932 \n",
      "Epoch: 6 Training loss: 23.371875348121627 \n",
      "Epoch: 7 Training loss: 22.8667521517414 \n",
      "Epoch: 8 Training loss: 22.40548612161486 \n",
      "Epoch: 9 Training loss: 21.998763328930462 \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainDataLoader:\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "          # Forward propagation\n",
    "          y_pred = model(images)\n",
    "\n",
    "          #compute the loss\n",
    "          loss = criterion(y_pred,labels)\n",
    "\n",
    "          # Calculating gradients\n",
    "          loss.backward()\n",
    "\n",
    "          # Update parameters\n",
    "          optimizer.step()\n",
    "\n",
    "        count += 1\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    else:\n",
    "        print(\"Epoch: {} Training loss: {} \".format(i, running_loss/len(trainDataLoader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sbr0bz3uo8Ou"
   },
   "source": [
    "## Visualize the Test Set\n",
    "- In this case, we try to get familiarize ourselves on how to use model for predicting 'out-of-training' samples and on how to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "-Sr5wNdnitqP",
    "outputId": "47841739-04fd-4a5a-bd35-8f705a732a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit: 8\n",
      "True Label: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12862cef0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testIter = iter(testDataLoader) #Convert Loader into built-in Python iterable\n",
    "\n",
    "test_images, test_labels = testIter.next() #Get the next element. Note: each element is has a dim of (batch_size,no_channels,width,height)\n",
    "\n",
    "# As I want to perform inference, disable gradient computation\n",
    "with torch.set_grad_enabled(False):  \n",
    "    y = model(test_images[0]) #Perform computation. Based on my model architecture, my expected output is 1,10\n",
    "    _, y_result = torch.max(y, 1) #Get the maximum along rows.\n",
    "\n",
    "print(\"Predicted digit: {}\".format(y_result[0]))\n",
    "print(\"True Label: {}\".format(test_labels[0]))\n",
    "plt.imshow(test_images[0].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6OOhN5queGU"
   },
   "source": [
    "## Evaluate our Model\n",
    "- We now use the whole testset to understand how well is our model.\n",
    "- For this one, we use accuracy as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JW8gpFCxpJEb",
    "outputId": "86a577ed-943b-40f8-d925-0ce8961202d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.6300\n"
     ]
    }
   ],
   "source": [
    "correct_counter = 0\n",
    "all_counter = 0\n",
    "model.eval()\n",
    "for test_images, test_labels in testDataLoader:\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y = model(test_images)\n",
    "\n",
    "        # Get highest value at column\n",
    "        _, preds = torch.max(y, 1)\n",
    "\n",
    "    correct_counter += torch.sum(preds == test_labels)\n",
    "    \n",
    "\n",
    "all_counter += len(testDataLoader.dataset)\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(correct_counter.double() / all_counter * 100))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2nRKMvcxxu7"
   },
   "source": [
    "At this point in time, we are able to complete an end-to-end machine learning project using Artificial Neual Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v2\n",
    "- Although the above training works well, it is highly recommended if you can add more insights from the model itself.\n",
    "- It would be easier for us to know if the model status (i.e. bias/variance) if we can see the training and testing loss for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/9\n",
      "Epoch: 0 Training Loss: 21.6124 Training Acc: 90.3450\n",
      "Epoch: 0 Validation loss: 20.6249 Val Acc: 90.6600\n",
      "epoch 1/9\n",
      "Epoch: 1 Training Loss: 21.2700 Training Acc: 90.4850\n",
      "Epoch: 1 Validation loss: 20.3244 Val Acc: 90.9600\n",
      "epoch 2/9\n",
      "Epoch: 2 Training Loss: 20.9437 Training Acc: 90.6033\n",
      "Epoch: 2 Validation loss: 20.0329 Val Acc: 90.9800\n",
      "epoch 3/9\n",
      "Epoch: 3 Training Loss: 20.6367 Training Acc: 90.7367\n",
      "Epoch: 3 Validation loss: 19.8326 Val Acc: 91.0100\n",
      "epoch 4/9\n",
      "Epoch: 4 Training Loss: 20.3676 Training Acc: 90.8100\n",
      "Epoch: 4 Validation loss: 19.4894 Val Acc: 91.0800\n",
      "epoch 5/9\n",
      "Epoch: 5 Training Loss: 20.0931 Training Acc: 90.9500\n",
      "Epoch: 5 Validation loss: 19.2312 Val Acc: 91.2700\n",
      "epoch 6/9\n",
      "Epoch: 6 Training Loss: 19.8312 Training Acc: 91.0350\n",
      "Epoch: 6 Validation loss: 19.0149 Val Acc: 91.3500\n",
      "epoch 7/9\n",
      "Epoch: 7 Training Loss: 19.5964 Training Acc: 91.2133\n",
      "Epoch: 7 Validation loss: 18.7947 Val Acc: 91.4500\n",
      "epoch 8/9\n",
      "Epoch: 8 Training Loss: 19.3666 Training Acc: 91.2683\n",
      "Epoch: 8 Validation loss: 18.6025 Val Acc: 91.5600\n",
      "epoch 9/9\n",
      "Epoch: 9 Training Loss: 19.1516 Training Acc: 91.4183\n",
      "Epoch: 9 Validation loss: 18.4114 Val Acc: 91.6800\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "  \n",
    "    print(\"epoch {}/{}\".format(i,epochs-1))\n",
    "\n",
    "    # Training phase\n",
    "    running_loss = 0\n",
    "    correct_counter = 0\n",
    "    all_counter = 0\n",
    "    model.train()\n",
    "    for images, labels in trainDataLoader:\n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Forward propagation\n",
    "            y_pred = model(images)\n",
    "\n",
    "            # Get highest value at column\n",
    "            _, preds = torch.max(y_pred, 1)\n",
    "\n",
    "            #compute the loss\n",
    "            loss = criterion(y_pred,labels)\n",
    "\n",
    "            # Calculating gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct_counter += torch.sum(preds == labels)\n",
    "\n",
    "    ave_loss = running_loss/len(trainDataLoader)\n",
    "    all_counter += len(trainDataLoader.dataset)  \n",
    "    accuracy = (correct_counter.double() / all_counter).item()\n",
    "\n",
    "    print(\"Epoch: {} Training Loss: {:.4f} Training Acc: {:.4f}\".format(i, ave_loss, accuracy*100))\n",
    "\n",
    "    # Evaluation Phase\n",
    "    correct_counter = 0\n",
    "    all_counter = 0\n",
    "    running_loss = 0 \n",
    "    model.eval()\n",
    "    for test_images, test_labels in testDataLoader:\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            y = model(test_images)\n",
    "\n",
    "            # Get highest value at column\n",
    "            _, preds = torch.max(y, 1)\n",
    "\n",
    "            #compute the loss\n",
    "            loss = criterion(y,test_labels)\n",
    "\n",
    "        running_loss += loss.item() * test_images.size(0)\n",
    "        correct_counter += torch.sum(preds == test_labels)\n",
    "\n",
    "    ave_loss = running_loss / len(testDataLoader)\n",
    "    all_counter += len(testDataLoader.dataset)\n",
    "    accuracy = (correct_counter.double() / all_counter).item()\n",
    "\n",
    "    print(\"Epoch: {} Validation loss: {:.4f} Val Acc: {:.4f}\".format(i, ave_loss, accuracy * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v2.1\n",
    "- Ealier, we developed the full training and test routine.\n",
    "- However, it's too lengthy and wordy. We can refactor it so that we can improve reusability and readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/9\n",
      "Epoch: 0 train Loss: 18.9371 Acc: 91.4633\n",
      "Epoch: 0 val Loss: 18.2302 Acc: 91.7100\n",
      "epoch 1/9\n",
      "Epoch: 1 train Loss: 18.7291 Acc: 91.5850\n",
      "Epoch: 1 val Loss: 18.0234 Acc: 91.8700\n",
      "epoch 2/9\n",
      "Epoch: 2 train Loss: 18.5378 Acc: 91.6700\n",
      "Epoch: 2 val Loss: 17.8852 Acc: 91.8500\n",
      "epoch 3/9\n",
      "Epoch: 3 train Loss: 18.3560 Acc: 91.7050\n",
      "Epoch: 3 val Loss: 17.6857 Acc: 91.9900\n",
      "epoch 4/9\n",
      "Epoch: 4 train Loss: 18.1639 Acc: 91.8467\n",
      "Epoch: 4 val Loss: 17.5239 Acc: 92.0200\n",
      "epoch 5/9\n",
      "Epoch: 5 train Loss: 17.9892 Acc: 91.9250\n",
      "Epoch: 5 val Loss: 17.3920 Acc: 92.0800\n",
      "epoch 6/9\n",
      "Epoch: 6 train Loss: 17.8282 Acc: 91.9533\n",
      "Epoch: 6 val Loss: 17.2497 Acc: 92.1900\n",
      "epoch 7/9\n",
      "Epoch: 7 train Loss: 17.6609 Acc: 92.0117\n",
      "Epoch: 7 val Loss: 17.0760 Acc: 92.2200\n",
      "epoch 8/9\n",
      "Epoch: 8 train Loss: 17.4938 Acc: 92.0967\n",
      "Epoch: 8 val Loss: 16.9600 Acc: 92.3800\n",
      "epoch 9/9\n",
      "Epoch: 9 train Loss: 17.3308 Acc: 92.2000\n",
      "Epoch: 9 val Loss: 16.8140 Acc: 92.3600\n"
     ]
    }
   ],
   "source": [
    "dataloader = {}\n",
    "dataloader['train'] = trainDataLoader\n",
    "dataloader['val'] = testDataLoader\n",
    "for i in range(epochs):\n",
    "  \n",
    "    print(\"epoch {}/{}\".format(i,epochs-1))\n",
    "    for phase in [\"train\",\"val\"]:\n",
    "        # Training phase\n",
    "        running_loss = 0\n",
    "        correct_counter = 0\n",
    "        all_counter = 0\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        for images, labels in dataloader[phase]:\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                # Forward propagation\n",
    "                y_pred = model(images)\n",
    "\n",
    "                # Get highest value at column\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "\n",
    "                #compute the loss\n",
    "                loss = criterion(y_pred,labels)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # Calculating gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            correct_counter += torch.sum(preds == labels)\n",
    "\n",
    "        ave_loss = running_loss/len(dataloader[phase])\n",
    "        all_counter += len(dataloader[phase].dataset)  \n",
    "        accuracy = (correct_counter.double() / all_counter).item()\n",
    "\n",
    "        print(\"Epoch: {} {} Loss: {:.4f} Acc: {:.4f}\".format(i, phase, ave_loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v3\n",
    "At this point, we want to make more reusable by doing the ff:\n",
    "    1. As training is very routinary, let's put a boilerplate `trainer`\n",
    "    2. As evaluation during training and validation is routinary, let's create an `evaluator`\n",
    "    3. Let's make the logging of results, extensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 0.2067 Acc: 94.0417\n",
      "Epoch: 1 Val Loss: 0.2089 Acc: 93.9500\n",
      "Epoch: 2 train Loss: 0.2047 Acc: 94.1400\n",
      "Epoch: 2 Val Loss: 0.2070 Acc: 94.0600\n",
      "Epoch: 3 train Loss: 0.2038 Acc: 94.0750\n",
      "Epoch: 3 Val Loss: 0.2061 Acc: 94.0100\n",
      "Epoch: 4 train Loss: 0.2020 Acc: 94.1500\n",
      "Epoch: 4 Val Loss: 0.2046 Acc: 94.0700\n",
      "Epoch: 5 train Loss: 0.2000 Acc: 94.2667\n",
      "Epoch: 5 Val Loss: 0.2030 Acc: 94.1400\n",
      "Epoch: 6 train Loss: 0.1988 Acc: 94.2750\n",
      "Epoch: 6 Val Loss: 0.2014 Acc: 94.1900\n",
      "Epoch: 7 train Loss: 0.1977 Acc: 94.2883\n",
      "Epoch: 7 Val Loss: 0.2001 Acc: 94.1700\n",
      "Epoch: 8 train Loss: 0.1956 Acc: 94.3500\n",
      "Epoch: 8 Val Loss: 0.1976 Acc: 94.2700\n",
      "Epoch: 9 train Loss: 0.1939 Acc: 94.4150\n",
      "Epoch: 9 Val Loss: 0.1971 Acc: 94.3100\n",
      "Epoch: 10 train Loss: 0.1929 Acc: 94.4817\n",
      "Epoch: 10 Val Loss: 0.1965 Acc: 94.4100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x128761978>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)})\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "\n",
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v4\n",
    "- At this point we are able to think of reusability and abstractions in mind\n",
    "- Lastly, if we have very expensive training (> 1 day), we want to add a plot so that we can easily see our progress\n",
    "- For this tutorial, we would utilize one of the famous plotting tools `Visdom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 0.1800 Acc: 94.8233\n",
      "Epoch: 1 Val Loss: 0.1850 Acc: 94.5800\n",
      "Epoch: 2 train Loss: 0.1785 Acc: 94.8517\n",
      "Epoch: 2 Val Loss: 0.1835 Acc: 94.6900\n",
      "Epoch: 3 train Loss: 0.1771 Acc: 94.9167\n",
      "Epoch: 3 Val Loss: 0.1826 Acc: 94.6900\n",
      "Epoch: 4 train Loss: 0.1756 Acc: 94.9633\n",
      "Epoch: 4 Val Loss: 0.1810 Acc: 94.8200\n",
      "Epoch: 5 train Loss: 0.1744 Acc: 94.9867\n",
      "Epoch: 5 Val Loss: 0.1797 Acc: 94.7400\n",
      "Epoch: 6 train Loss: 0.1736 Acc: 95.0117\n",
      "Epoch: 6 Val Loss: 0.1786 Acc: 94.8300\n",
      "Epoch: 7 train Loss: 0.1719 Acc: 95.0600\n",
      "Epoch: 7 Val Loss: 0.1775 Acc: 94.8000\n",
      "Epoch: 8 train Loss: 0.1707 Acc: 95.1483\n",
      "Epoch: 8 Val Loss: 0.1762 Acc: 94.9000\n",
      "Epoch: 9 train Loss: 0.1697 Acc: 95.1050\n",
      "Epoch: 9 Val Loss: 0.1750 Acc: 94.8200\n",
      "Epoch: 10 train Loss: 0.1685 Acc: 95.2050\n",
      "Epoch: 10 Val Loss: 0.1744 Acc: 94.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x12a369f60>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from visdom import Visdom\n",
    "\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)})\n",
    "vis = Visdom()\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "#Initialize all the plot windows we wanted\n",
    "train_loss_window = create_plot_window(vis,'#Iterations','Loss','Training Avg Loss')\n",
    "val_loss_window = create_plot_window(vis,'#Iterations','Loss','Val Avg Loss')\n",
    "train_acc_window = create_plot_window(vis,'#Iterations','Accuracy','Training Avg Accuracy')\n",
    "val_acc_window = create_plot_window(vis,'#Iterations','Accuracy','Val Avg Accuracy')\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_loss]),win=train_loss_window,update='append')\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_accuracy]),win=train_acc_window,update='append')\n",
    "    \n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_loss]),win=val_loss_window,update='append')\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_accuracy]),win=val_acc_window,update='append')\n",
    "\n",
    "\n",
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNv0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
