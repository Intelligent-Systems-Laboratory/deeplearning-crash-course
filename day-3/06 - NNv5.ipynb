{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiiKgI6Do5n"
   },
   "source": [
    "# End-To-End Neural Network in Pytorch (GPU)\n",
    "- Now, we want to make it flexible such that it can use CPU or GPU (if it's available)\n",
    "- In order for us to do so, we simply have to think of three things:\n",
    "    1. Loading Data and Labels to GPU during training\n",
    "    2. Loading Data and Lables to GPU during evaluation\n",
    "    3. Loading Models into GPU\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Check if CUDA at slot 0 is available or not. Otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: drive/My Drive/mnist\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5c2c65fc18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "            In the constructor we instantiate three nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            Note: we just define components. we have not yet connected them. \n",
    "            If you think of it like a graph, we are only declaring nodes, we have not declared edges\n",
    "\n",
    "            Input:\n",
    "            D_in: a scalar value that defines the no. of neurons on layer 0 (a.k.a. input layer)\n",
    "            H1: number of neurons in layer 1\n",
    "            H2: number of neurons in layer 2\n",
    "            D_out: number of neurons in layer 3\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__() #Call the parent class\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in,H1) # Linear => Linear Transformation. In equation, this is the same as Wx + b.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H1,H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H2,D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Softmax is simply sigmoid for multiple classes. That is, if we only have two classes, softmax == sigmoid.\n",
    "        #Note: We need softmax/sigmoid at the end if it's a classification task. If not, we skip this step.\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"\"\"\n",
    "          This is where you define your forward propagation.\n",
    "          In other words, this is where we combine and connect all the components we defined above.\n",
    "\n",
    "          Input:\n",
    "          x0 = the actual image in batch\n",
    "\n",
    "        \"\"\"\n",
    "        # Method View = reshape the tensor into a different dimension.\n",
    "        # x0.shape[0] = we get the first dim of the image shape. Recall: This would be batch_size\n",
    "        # -1 = The -1 as the second argument means compute the remaining and that would be my second dimension.\n",
    "        # Thus, the whole line below means, I want to reshape this batch of images by doing the ff:\n",
    "        # The first dim of this new 'view' would be based on the first dim of this image. In this case, I would get 64\n",
    "        # The second dim of this new 'view' would be computed based on the remaining pixels left. Thus, it would be 28*28=784.\n",
    "        # Thus, The new view would be (64,784)\n",
    "        x0 = x0.view(x0.shape[0],-1)\n",
    "        x1 = self.relu1(self.linear1(x0))\n",
    "        x2 = self.relu2(self.linear2(x1))\n",
    "        x3 = self.softmax(self.linear3(x2)) \n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "model = MLP(D_in,H1,H2,D_out).to(device) #Note: We transferred it to GPU\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tenorboard as Visualizer Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mnist_run_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v4.1\n",
    "- At this point we are able to think of reusability and abstractions in mind\n",
    "- We are able to add plot using Visdom\n",
    "- We now have to transfer it to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "#We transferred both the model and data to GPU on trainer and evaluator\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion,device)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)},device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    writer.add_scalar('training loss', avg_loss, epoch * len(trainDataLoader))\n",
    "    writer.add_scalar('training accuracy', avg_accuracy, epoch * len(trainDataLoader))\n",
    "    \n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    writer.add_scalar('validation loss', avg_loss, epoch * len(testDataLoader))\n",
    "    writer.add_scalar('validation accuracy', avg_accuracy, epoch * len(testDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 2.1273 Acc: 52.0083\n",
      "Epoch: 1 Val Loss: 2.1230 Acc: 52.6300\n",
      "Epoch: 2 train Loss: 1.7639 Acc: 60.5883\n",
      "Epoch: 2 Val Loss: 1.7521 Acc: 61.3100\n",
      "Epoch: 3 train Loss: 1.2690 Acc: 71.6333\n",
      "Epoch: 3 Val Loss: 1.2523 Acc: 72.4700\n",
      "Epoch: 4 train Loss: 0.9288 Acc: 77.7183\n",
      "Epoch: 4 Val Loss: 0.9103 Acc: 78.3800\n",
      "Epoch: 5 train Loss: 0.7468 Acc: 80.3550\n",
      "Epoch: 5 Val Loss: 0.7283 Acc: 81.0100\n",
      "Epoch: 6 train Loss: 0.6412 Acc: 82.4783\n",
      "Epoch: 6 Val Loss: 0.6231 Acc: 83.2500\n",
      "Epoch: 7 train Loss: 0.5722 Acc: 84.2417\n",
      "Epoch: 7 Val Loss: 0.5532 Acc: 84.7900\n",
      "Epoch: 8 train Loss: 0.5241 Acc: 85.4350\n",
      "Epoch: 8 Val Loss: 0.5055 Acc: 86.1000\n",
      "Epoch: 9 train Loss: 0.4869 Acc: 86.4783\n",
      "Epoch: 9 Val Loss: 0.4686 Acc: 87.1000\n",
      "Epoch: 10 train Loss: 0.4586 Acc: 87.2283\n",
      "Epoch: 10 Val Loss: 0.4405 Acc: 87.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f5c24438940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNv0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
