{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWiiKgI6Do5n"
   },
   "source": [
    "# End-To-End Neural Network in Pytorch (GPU)\n",
    "- Now, we want to make it flexible such that it can use CPU or GPU (if it's available)\n",
    "- In order for us to do so, we simply have to think of three things:\n",
    "    1. Loading Data and Labels to GPU during training\n",
    "    2. Loading Data and Lables to GPU during evaluation\n",
    "    3. Loading Models into GPU\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Check if CUDA at slot 0 is available or not. Otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: drive/My Drive/mnist\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125ab4f28>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, D_out):\n",
    "        \"\"\"\n",
    "            In the constructor we instantiate three nn.Linear modules and assign them as\n",
    "            member variables.\n",
    "            Note: we just define components. we have not yet connected them. \n",
    "            If you think of it like a graph, we are only declaring nodes, we have not declared edges\n",
    "\n",
    "            Input:\n",
    "            D_in: a scalar value that defines the no. of neurons on layer 0 (a.k.a. input layer)\n",
    "            H1: number of neurons in layer 1\n",
    "            H2: number of neurons in layer 2\n",
    "            D_out: number of neurons in layer 3\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__() #Call the parent class\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in,H1) # Linear => Linear Transformation. In equation, this is the same as Wx + b.\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(H1,H2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(H2,D_out)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Softmax is simply sigmoid for multiple classes. That is, if we only have two classes, softmax == sigmoid.\n",
    "        #Note: We need softmax/sigmoid at the end if it's a classification task. If not, we skip this step.\n",
    "\n",
    "    def forward(self, x0):\n",
    "        \"\"\"\n",
    "          This is where you define your forward propagation.\n",
    "          In other words, this is where we combine and connect all the components we defined above.\n",
    "\n",
    "          Input:\n",
    "          x0 = the actual image in batch\n",
    "\n",
    "        \"\"\"\n",
    "        # Method View = reshape the tensor into a different dimension.\n",
    "        # x0.shape[0] = we get the first dim of the image shape. Recall: This would be batch_size\n",
    "        # -1 = The -1 as the second argument means compute the remaining and that would be my second dimension.\n",
    "        # Thus, the whole line below means, I want to reshape this batch of images by doing the ff:\n",
    "        # The first dim of this new 'view' would be based on the first dim of this image. In this case, I would get 64\n",
    "        # The second dim of this new 'view' would be computed based on the remaining pixels left. Thus, it would be 28*28=784.\n",
    "        # Thus, The new view would be (64,784)\n",
    "        x0 = x0.view(x0.shape[0],-1)\n",
    "        x1 = self.relu1(self.linear1(x0))\n",
    "        x2 = self.relu2(self.linear2(x1))\n",
    "        x3 = self.softmax(self.linear3(x2)) \n",
    "\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "model = MLP(D_in,H1,H2,D_out).to(device) #Note: We transferred it to GPU\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v4\n",
    "- At this point we are able to think of reusability and abstractions in mind\n",
    "- We are able to add plot using Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 2.1273 Acc: 52.0100\n",
      "Epoch: 1 Val Loss: 2.1230 Acc: 52.6500\n",
      "Epoch: 2 train Loss: 1.7639 Acc: 60.5900\n",
      "Epoch: 2 Val Loss: 1.7521 Acc: 61.3200\n",
      "Epoch: 3 train Loss: 1.2690 Acc: 71.6350\n",
      "Epoch: 3 Val Loss: 1.2523 Acc: 72.4600\n",
      "Epoch: 4 train Loss: 0.9287 Acc: 77.7217\n",
      "Epoch: 4 Val Loss: 0.9103 Acc: 78.3800\n",
      "Epoch: 5 train Loss: 0.7468 Acc: 80.3583\n",
      "Epoch: 5 Val Loss: 0.7283 Acc: 81.0100\n",
      "Epoch: 6 train Loss: 0.6412 Acc: 82.4767\n",
      "Epoch: 6 Val Loss: 0.6231 Acc: 83.2500\n",
      "Epoch: 7 train Loss: 0.5722 Acc: 84.2417\n",
      "Epoch: 7 Val Loss: 0.5532 Acc: 84.7800\n",
      "Epoch: 8 train Loss: 0.5241 Acc: 85.4383\n",
      "Epoch: 8 Val Loss: 0.5055 Acc: 86.1000\n",
      "Epoch: 9 train Loss: 0.4869 Acc: 86.4783\n",
      "Epoch: 9 Val Loss: 0.4686 Acc: 87.1100\n",
      "Epoch: 10 train Loss: 0.4586 Acc: 87.2283\n",
      "Epoch: 10 Val Loss: 0.4405 Acc: 87.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x12921ff98>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from visdom import Visdom\n",
    "\n",
    "#We transferred both the model and data to GPU on trainer and evaluator\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion,device)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)},device=device)\n",
    "vis = Visdom()\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "#Initialize all the plot windows we wanted\n",
    "train_loss_window = create_plot_window(vis,'#Iterations','Loss','Training Avg Loss')\n",
    "val_loss_window = create_plot_window(vis,'#Iterations','Loss','Val Avg Loss')\n",
    "train_acc_window = create_plot_window(vis,'#Iterations','Accuracy','Training Avg Accuracy')\n",
    "val_acc_window = create_plot_window(vis,'#Iterations','Accuracy','Val Avg Accuracy')\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_loss]),win=train_loss_window,update='append')\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_accuracy]),win=train_acc_window,update='append')\n",
    "    \n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_loss]),win=val_loss_window,update='append')\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_accuracy]),win=val_acc_window,update='append')\n",
    "\n",
    "\n",
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNv0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
