{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "- Now that we've understood the whole framework of training in Pytorch\n",
    "- We can now focus on improving the architecture\n",
    "- Before we dive deep into CNN, We've outline some guide questions for you to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What's the biggest problem of Multi-Layer Perceptron if we are going to extend into more than one hidden layer?\n",
    "\n",
    "2. What's the innovation of Convolutional Neural Network?\n",
    "\n",
    "3. What's a 2D convolution anyway?\n",
    "\n",
    "4. When we say kernel size of 3, what does it mean?\n",
    "\n",
    "5. What do we mean by channel?\n",
    "\n",
    "6. How about depth?\n",
    "\n",
    "7. What is padding?\n",
    "\n",
    "8. What is stride?\n",
    "\n",
    "9. How do we ensure that the output map would have the same size with the input?\n",
    "\n",
    "10. If you are given input volume of 10x128x128, you convolved it with 1x3x3 (i.e. no. of channel first!) with Padding of 1, and Stride of 1, What is the expected output volume?\n",
    "\n",
    "11. If you are given input volume of 3x227x227, you convolved it with 96x11x11 (i.e. no. of channel first!) with Padding of 0, and Stride of 4, What is the expected output volume?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Check if CUDA at slot 0 is available or not. Otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: drive/My Drive/mnist\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8bec61ff28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1) #No. of input channel,No. of output channel, Kernel Size,stride\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 4*4*50) # reshape\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        \n",
    "        x = self.softmax(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "#model = MLP(D_in,H1,H2,D_out).to(device) #Note: We transferred it to GPU\n",
    "\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "# criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Tenorboard as Visualizer Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('runs/mnist_run_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v4\n",
    "- At this point we are able to think of reusability and abstractions in mind\n",
    "- For this tutorial, we would utilize the most famous plotting tool `Tensorboard`\n",
    "- We now have to transfer it to GPU\n",
    "- We showcase that these sets of code is reusable on a different Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "#We transferred both the model and data to GPU on trainer and evaluator\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion,device)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)},device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    writer.add_scalar('training loss', avg_loss, epoch * len(trainDataLoader))\n",
    "    writer.add_scalar('training accuracy', avg_accuracy, epoch * len(trainDataLoader))\n",
    "    \n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    writer.add_scalar('validation loss', avg_loss, epoch * len(testDataLoader))\n",
    "    writer.add_scalar('validation accuracy', avg_accuracy, epoch * len(testDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 2.2856 Acc: 33.8933\n",
      "Epoch: 1 Val Loss: 2.2852 Acc: 34.7200\n",
      "Epoch: 2 train Loss: 2.2809 Acc: 30.5250\n",
      "Epoch: 2 Val Loss: 2.2804 Acc: 31.2600\n",
      "Epoch: 3 train Loss: 2.2734 Acc: 26.5767\n",
      "Epoch: 3 Val Loss: 2.2728 Acc: 27.0400\n",
      "Epoch: 4 train Loss: 2.2598 Acc: 23.1467\n",
      "Epoch: 4 Val Loss: 2.2588 Acc: 23.2800\n",
      "Epoch: 5 train Loss: 2.2326 Acc: 25.3633\n",
      "Epoch: 5 Val Loss: 2.2312 Acc: 25.7300\n",
      "Epoch: 6 train Loss: 2.1842 Acc: 31.8567\n",
      "Epoch: 6 Val Loss: 2.1822 Acc: 32.1600\n",
      "Epoch: 7 train Loss: 2.1178 Acc: 38.3567\n",
      "Epoch: 7 Val Loss: 2.1151 Acc: 38.4700\n",
      "Epoch: 8 train Loss: 2.0372 Acc: 53.1700\n",
      "Epoch: 8 Val Loss: 2.0336 Acc: 53.3000\n",
      "Epoch: 9 train Loss: 1.9419 Acc: 62.7733\n",
      "Epoch: 9 Val Loss: 1.9371 Acc: 62.9400\n",
      "Epoch: 10 train Loss: 1.8773 Acc: 64.3817\n",
      "Epoch: 10 Val Loss: 1.8724 Acc: 64.7200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7f8be4259a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
