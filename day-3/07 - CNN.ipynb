{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "- Now that we've understood the whole framework of training in Pytorch\n",
    "- We can now focus on improving the architecture\n",
    "- Before we dive deep into CNN, We've outline some guide questions for you to answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What's the biggest problem of Multi-Layer Perceptron if we are going to extend into more than one hidden layer?\n",
    "\n",
    "2. What's the innovation of Convolutional Neural Network?\n",
    "\n",
    "3. What's a 2D convolution anyway?\n",
    "\n",
    "4. When we say kernel size of 3, what does it mean?\n",
    "\n",
    "5. What do we mean by channel?\n",
    "\n",
    "6. How about depth?\n",
    "\n",
    "7. What is padding?\n",
    "\n",
    "8. What is stride?\n",
    "\n",
    "9. How do we ensure that the output map would have the same size with the input?\n",
    "\n",
    "10. If you are given input volume of 10x128x128, you convolved it with 1x3x3 (i.e. no. of channel first!) with Padding of 1, and Stride of 1, What is the expected output volume?\n",
    "\n",
    "11. If you are given input volume of 3x227x227, you convolved it with 96x11x11 (i.e. no. of channel first!) with Padding of 0, and Stride of 4, What is the expected output volume?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q4PhJas2D9sD"
   },
   "source": [
    "## Load Typical Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02Os2k3yTZRH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHXZKghFWdNR"
   },
   "outputs": [],
   "source": [
    "# For colab users\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8wjZMgVEFga"
   },
   "source": [
    "## Gather your dataset\n",
    "- In this case, we want MNIST dataset.\n",
    "- Fortunately, this is already built-in from torchvision package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "K3at2DF_XGrL",
    "outputId": "bf6ea43a-9611-4c51-f4af-cf9ec2c6a5de"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Check if CUDA at slot 0 is available or not. Otherwise use CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Define a transform to augment the data. \n",
    "# Transform #1 = we want to convert everything into tensor format so that we can streamline pytorch packages\n",
    "# Transform #2 = Normalize. This is used for data cleaning so that you can help the model perform better.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "#Load your MNIST Dataset\n",
    "# If you check the documentation of MNIST method, \n",
    "#              argument #1 = directory to save the fetched dataset\n",
    "#              argument #2 (download)  = do we download if you don't have on the said directory?\n",
    "#              argument #3 (train)     = what type of dataset?\n",
    "#              argument #4 (transform) = which transformation do you like to do? \n",
    "train_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('drive/My Drive/mnist', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMF9FxJyFCd8"
   },
   "source": [
    "We then try to check what does it look it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Gr9enBtPZuU_",
    "outputId": "488a86ab-06a5-4871-9633-9ab6bd587744"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: drive/My Drive/mnist\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.5,), std=(0.5,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1kJZKnFHKE"
   },
   "source": [
    "From above, we see that the `Dataset` object is like a folder structure. It provides you a mechanism to see its properties. \n",
    "\n",
    "*Recall*: In our previous lesson, we showed that training models in pytorch was done using the built-in `for` loop of Python. Our dataset that time was just using list of random numbers. \n",
    "\n",
    "But now that we are using real-world dataset via `Dataset` object. So how do we use it for model training?\n",
    "\n",
    "We can use `DataLoader` utility to convert the dataset object into an `iterable` object which you can use within `for` loop later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgyBIzCZX3Tk"
   },
   "outputs": [],
   "source": [
    "# This is a function for you to change it from the dataset structure above into something iterable for the looping statement later on.\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sieoj6J8aDqc"
   },
   "source": [
    "Let's try to check what does one iteration look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yPdO7nflY6k6",
    "outputId": "f61c750a-11e3-4f61-86e9-b90816e1ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainIter = iter(trainDataLoader) # Convert it into python iterable built-in object\n",
    "\n",
    "images, labels = trainIter.next() #Get the next element\n",
    "\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZitd5igHSpT"
   },
   "source": [
    "As you can see, torch dimensional format is as follows:  \n",
    "dim0 = batch size  (i.e. number of images in one batch)  \n",
    "dim1 = no. of channels  (i.e. number of color channel if you may, e.g. R,G,B)  \n",
    "dim2 = width  (i.e. how wide the photo in pixel)  \n",
    "dim3 = height (i.e. how tall the photo in pixel)   \n",
    "\n",
    "Since the no. of channel is only one, we see that this is 'black-and-white' photo. It is not colored. Technically, this is called a grayscale image.\n",
    "\n",
    "Note: If you are coming from tensorflow, they have different arrangement of dimension esp no. of channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVRQbVMgJ0zE"
   },
   "source": [
    "## Visualize the dataset\n",
    "This is how our dataset looks like. Don't forget than a grayscale image is 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "gFaCmXXHHOLN",
    "outputId": "a79514df-7cdc-4f41-fda1-8c92b0cd5432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11ee78e48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.imshow(images[0].squeeze()) #Note: We get the image 0 from the batch. Thus, it becomes 3 dim. We would 'squeeze' so that it becomes 2 dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rPCEbzGKwK7"
   },
   "source": [
    "## NN Module\n",
    "\n",
    "- this is where we define our Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbBHGKbCZdr9"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,20,5,1) #No. of input channel,No. of output channel, Kernel Size,stride\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(20,50,5,1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 4*4*50) # reshape\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        \n",
    "        x = self.softmax(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0NycTT3Kll2"
   },
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lio8wmX5fFOA"
   },
   "outputs": [],
   "source": [
    "seed = 24 #Fave Number. Kobe == 24\n",
    "torch.manual_seed(seed) # This is set in order to have reproducible and comparable results\n",
    "\n",
    "\n",
    "D_in = 784 # Number of input neuron in layer 0 (input layer)\n",
    "H1 = 150   # Number of neuron in layer 1\n",
    "H2 = 48    # number of neuron in layer 2\n",
    "D_out = 10 # number of neuron in layer 3 (output layer)\n",
    "\n",
    "#model = MLP(D_in,H1,H2,D_out).to(device) #Note: We transferred it to GPU\n",
    "\n",
    "model = LeNet()\n",
    "\n",
    "\n",
    "epochs = 10 # Number of times it would repeat the whole training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smFSQ_3QgLR9"
   },
   "outputs": [],
   "source": [
    "# Define the cost function\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')\n",
    "# criterion = nn.NLLLoss() #This is good for classification task\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define how we would update the weights\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.0001,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation Framework v4\n",
    "- At this point we are able to think of reusability and abstractions in mind\n",
    "- We are able to add plot using Visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train Loss: 2.3016 Acc: 11.6417\n",
      "Epoch: 1 Val Loss: 2.3016 Acc: 11.7400\n",
      "Epoch: 2 train Loss: 2.3008 Acc: 15.5950\n",
      "Epoch: 2 Val Loss: 2.3008 Acc: 16.0200\n",
      "Epoch: 3 train Loss: 2.2999 Acc: 23.7050\n",
      "Epoch: 3 Val Loss: 2.2998 Acc: 23.9700\n",
      "Epoch: 4 train Loss: 2.2989 Acc: 29.1367\n",
      "Epoch: 4 Val Loss: 2.2988 Acc: 29.5700\n",
      "Epoch: 5 train Loss: 2.2978 Acc: 32.1600\n",
      "Epoch: 5 Val Loss: 2.2977 Acc: 32.6200\n",
      "Epoch: 6 train Loss: 2.2966 Acc: 34.5850\n",
      "Epoch: 6 Val Loss: 2.2965 Acc: 35.6700\n",
      "Epoch: 7 train Loss: 2.2952 Acc: 36.8383\n",
      "Epoch: 7 Val Loss: 2.2951 Acc: 37.8700\n",
      "Epoch: 8 train Loss: 2.2935 Acc: 38.6117\n",
      "Epoch: 8 Val Loss: 2.2933 Acc: 39.8500\n",
      "Epoch: 9 train Loss: 2.2915 Acc: 38.3100\n",
      "Epoch: 9 Val Loss: 2.2913 Acc: 39.0500\n",
      "Epoch: 10 train Loss: 2.2889 Acc: 36.6650\n",
      "Epoch: 10 Val Loss: 2.2887 Acc: 37.3700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x11f7a9e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from visdom import Visdom\n",
    "\n",
    "#We transferred both the model and data to GPU on trainer and evaluator\n",
    "trainer = create_supervised_trainer(model,optimizer,criterion,device)\n",
    "evaluator = create_supervised_evaluator(model,metrics={'accuracy':Accuracy(),\n",
    "                                                      'loss':Loss(criterion)},device=device)\n",
    "vis = Visdom()\n",
    "\n",
    "def create_plot_window(vis, xlabel, ylabel, title):\n",
    "    return vis.line(X=np.array([1]), Y=np.array([np.nan]), opts=dict(xlabel=xlabel, ylabel=ylabel, title=title))\n",
    "\n",
    "#Initialize all the plot windows we wanted\n",
    "train_loss_window = create_plot_window(vis,'#Iterations','Loss','Training Avg Loss')\n",
    "val_loss_window = create_plot_window(vis,'#Iterations','Loss','Val Avg Loss')\n",
    "train_acc_window = create_plot_window(vis,'#Iterations','Accuracy','Training Avg Accuracy')\n",
    "val_acc_window = create_plot_window(vis,'#Iterations','Accuracy','Val Avg Accuracy')\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(engine):\n",
    "    evaluator.run(trainDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "    print(\"Epoch: {} train Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_loss]),win=train_loss_window,update='append')\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_accuracy]),win=train_acc_window,update='append')\n",
    "    \n",
    "    pass\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(testDataLoader)\n",
    "    avg_accuracy = evaluator.state.metrics['accuracy']\n",
    "    avg_loss = evaluator.state.metrics['loss']\n",
    "    epoch = engine.state.epoch\n",
    "\n",
    "    print(\"Epoch: {} Val Loss: {:.4f} Acc: {:.4f}\".format(epoch, avg_loss, avg_accuracy*100))\n",
    "    \n",
    "    # Update your plots\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_loss]),win=val_loss_window,update='append')\n",
    "    vis.line(X=np.array([epoch]),Y=np.array([avg_accuracy]),win=val_acc_window,update='append')\n",
    "\n",
    "\n",
    "trainer.run(trainDataLoader, max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
