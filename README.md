# Deep Learning  Crash Course

This crash course believes in three philosophies:

The first one is:
> We learn by doing

Thus, we emphasize this philosophy by hopefully doing everything as hands-on as possible.

The second one is: 
> We are our best teachers

This means that we learn best when we learn it on our own. Based on my twenty six years of existence, the best learners I have met learns by exploring it on their own. They pursue knowledge for knowledge sake. 

Hence, in this workshop, what we want to emphasize is that the materials should be exploratory on its own. You are actually encouraged to use more reference material other than the ones contained here. 

In corollary to this, as most of the best materials are in text format. This course focuses on you becoming a reader. 

The third one is:
> We will never be ready

This means that you don't need so much pre-requisites to learn something new. 

Thus, the following are not a valid excuse:  
- *"Oh! I never learn python, so I can't do the exercise."*
- *"I am not good in programming, so this is too fast. I can't do it. "*
- *"I can't do this. How can I do it without having formal lectures?"*

The point here is as long as you can do three things:      
(1) Use your laptop for reading and writing,    
(2) Use your mouse/trackpad for navigating on your screen   
(3) Type something using your keyboard,    

then you can do it!

Last but not the least, I just want to pre-empt you that you might feel very uncomfortable as there's so many complicated things to learn in one sitting. 

So, I just want to let you know:   
**"Welcome to the world of software development... where every thing is complicated!"**

**Carpe Diem!!**

## Day 2 - Machine Learning and Neural Networks

### Recap of Reading List:

### Neural Network. Multi-Layer Perceptron

Recap: Three Blue One Brown
- How does Neural Network understand a handwritten digit image?
- How does he learn the weight?
- In your own words, explain the process of learning and why is it significant?
- What happens to a two-layer network when you remove the activation/"squishing" function?
- In the paradigm of neural network, the nickname of logistic regression is?

### Big Picture
Similar to our morning exercise, let's focus on the big picture.

[Tensorflow Playground](https://playground.tensorflow.org/)   
[Understanding Neural Network](https://cloud.google.com/blog/products/gcp/understanding-neural-networks-with-tensorflow-playground)

[Test Your Understanding](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/playground-exercises)

### Neural Network in Pytorch

This is the part where we would now implement Neural Network
We start first from numpy then we progress into Pytorch

Note we are doing the same thing. But we progressively add features on it.

[Neural Network in Numpy](03_two_layer_net_numpy.ipynb)   
[Neural Network in Tensor](04_two_layer_net_tensor.ipynb)   
[Neural Network with Autogradient](05_two_layer_net_autograd.ipynb)   
[Neural Network with NN Module](07_two_layer_net_module.ipynb)   
[Neural Network with Optim Module](08_two_layer_net_optim.ipynb)  
[Neural Network with Custom NN](09_two_layer_net_custom_function)



## Old Reference
### [Day 1: Data Science and Data Manipulation using Python](day-1/README.md)