{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining New autograd Functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28665102.0\n",
      "1 22242076.0\n",
      "2 19175632.0\n",
      "3 16746028.0\n",
      "4 13989446.0\n",
      "5 10927047.0\n",
      "6 7988110.5\n",
      "7 5575290.5\n",
      "8 3800734.75\n",
      "9 2593030.75\n",
      "10 1802356.0\n",
      "11 1292487.375\n",
      "12 960795.75\n",
      "13 740016.8125\n",
      "14 588100.375\n",
      "15 479581.5\n",
      "16 399223.8125\n",
      "17 337492.25\n",
      "18 288598.75\n",
      "19 248962.5625\n",
      "20 216266.9375\n",
      "21 188897.28125\n",
      "22 165741.390625\n",
      "23 145974.296875\n",
      "24 129011.5078125\n",
      "25 114325.6796875\n",
      "26 101566.09375\n",
      "27 90450.0078125\n",
      "28 80721.2109375\n",
      "29 72187.1171875\n",
      "30 64682.09765625\n",
      "31 58066.81640625\n",
      "32 52219.3515625\n",
      "33 47041.6953125\n",
      "34 42445.73046875\n",
      "35 38358.31640625\n",
      "36 34722.578125\n",
      "37 31477.57421875\n",
      "38 28577.6953125\n",
      "39 25977.51171875\n",
      "40 23645.791015625\n",
      "41 21551.630859375\n",
      "42 19668.943359375\n",
      "43 17975.5078125\n",
      "44 16449.666015625\n",
      "45 15070.86328125\n",
      "46 13823.9453125\n",
      "47 12694.0576171875\n",
      "48 11669.1298828125\n",
      "49 10738.884765625\n",
      "50 9892.7880859375\n",
      "51 9123.10546875\n",
      "52 8421.263671875\n",
      "53 7780.97802734375\n",
      "54 7196.2490234375\n",
      "55 6661.98828125\n",
      "56 6173.39013671875\n",
      "57 5725.5732421875\n",
      "58 5314.638671875\n",
      "59 4937.32958984375\n",
      "60 4590.6298828125\n",
      "61 4272.0654296875\n",
      "62 3978.78173828125\n",
      "63 3708.47509765625\n",
      "64 3458.9755859375\n",
      "65 3228.594482421875\n",
      "66 3015.484619140625\n",
      "67 2818.3701171875\n",
      "68 2636.0478515625\n",
      "69 2466.97998046875\n",
      "70 2310.052001953125\n",
      "71 2164.40478515625\n",
      "72 2029.0439453125\n",
      "73 1903.1640625\n",
      "74 1786.0699462890625\n",
      "75 1677.04638671875\n",
      "76 1575.4268798828125\n",
      "77 1480.6876220703125\n",
      "78 1392.2841796875\n",
      "79 1309.762939453125\n",
      "80 1232.6749267578125\n",
      "81 1160.6119384765625\n",
      "82 1093.205810546875\n",
      "83 1030.14306640625\n",
      "84 971.1228637695312\n",
      "85 915.7916870117188\n",
      "86 863.9411010742188\n",
      "87 815.3164672851562\n",
      "88 769.7069702148438\n",
      "89 726.8906860351562\n",
      "90 686.6781005859375\n",
      "91 648.900634765625\n",
      "92 613.3888549804688\n",
      "93 580.006103515625\n",
      "94 548.5817260742188\n",
      "95 519.0100708007812\n",
      "96 491.1621398925781\n",
      "97 464.93634033203125\n",
      "98 440.2295227050781\n",
      "99 416.94427490234375\n",
      "100 394.9867248535156\n",
      "101 374.2784423828125\n",
      "102 354.74639892578125\n",
      "103 336.2960205078125\n",
      "104 318.87945556640625\n",
      "105 302.4283142089844\n",
      "106 286.8905944824219\n",
      "107 272.20184326171875\n",
      "108 258.3233947753906\n",
      "109 245.19677734375\n",
      "110 232.79000854492188\n",
      "111 221.04559326171875\n",
      "112 209.93045043945312\n",
      "113 199.4102325439453\n",
      "114 189.45150756835938\n",
      "115 180.01988220214844\n",
      "116 171.0836639404297\n",
      "117 162.6215057373047\n",
      "118 154.6038055419922\n",
      "119 147.00352478027344\n",
      "120 139.79676818847656\n",
      "121 132.96400451660156\n",
      "122 126.48260498046875\n",
      "123 120.33403015136719\n",
      "124 114.50245666503906\n",
      "125 108.96884155273438\n",
      "126 103.7159423828125\n",
      "127 98.72845458984375\n",
      "128 93.99421691894531\n",
      "129 89.49838256835938\n",
      "130 85.22537994384766\n",
      "131 81.16824340820312\n",
      "132 77.31443786621094\n",
      "133 73.6506118774414\n",
      "134 70.16830444335938\n",
      "135 66.8607406616211\n",
      "136 63.71671676635742\n",
      "137 60.72520065307617\n",
      "138 57.88042449951172\n",
      "139 55.17323684692383\n",
      "140 52.59852600097656\n",
      "141 50.149940490722656\n",
      "142 47.819908142089844\n",
      "143 45.603118896484375\n",
      "144 43.49210739135742\n",
      "145 41.48352813720703\n",
      "146 39.57195281982422\n",
      "147 37.750709533691406\n",
      "148 36.01723861694336\n",
      "149 34.36570739746094\n",
      "150 32.792842864990234\n",
      "151 31.29541015625\n",
      "152 29.868255615234375\n",
      "153 28.508560180664062\n",
      "154 27.212642669677734\n",
      "155 25.979175567626953\n",
      "156 24.802616119384766\n",
      "157 23.680885314941406\n",
      "158 22.61153793334961\n",
      "159 21.59315299987793\n",
      "160 20.621177673339844\n",
      "161 19.694190979003906\n",
      "162 18.810894012451172\n",
      "163 17.96822166442871\n",
      "164 17.163854598999023\n",
      "165 16.39813995361328\n",
      "166 15.666470527648926\n",
      "167 14.969460487365723\n",
      "168 14.30379581451416\n",
      "169 13.66812515258789\n",
      "170 13.062005996704102\n",
      "171 12.484857559204102\n",
      "172 11.934160232543945\n",
      "173 11.408354759216309\n",
      "174 10.907032012939453\n",
      "175 10.42760944366455\n",
      "176 9.970194816589355\n",
      "177 9.533348083496094\n",
      "178 9.116325378417969\n",
      "179 8.717945098876953\n",
      "180 8.337093353271484\n",
      "181 7.9737629890441895\n",
      "182 7.6267781257629395\n",
      "183 7.294781684875488\n",
      "184 6.978083610534668\n",
      "185 6.674934387207031\n",
      "186 6.385754108428955\n",
      "187 6.109428882598877\n",
      "188 5.84504508972168\n",
      "189 5.592574596405029\n",
      "190 5.351289749145508\n",
      "191 5.120552062988281\n",
      "192 4.899736404418945\n",
      "193 4.689244270324707\n",
      "194 4.487483978271484\n",
      "195 4.294941425323486\n",
      "196 4.110599994659424\n",
      "197 3.9345543384552\n",
      "198 3.7662298679351807\n",
      "199 3.6050775051116943\n",
      "200 3.451127529144287\n",
      "201 3.3033854961395264\n",
      "202 3.162527561187744\n",
      "203 3.0278429985046387\n",
      "204 2.8989789485931396\n",
      "205 2.7757561206817627\n",
      "206 2.657815933227539\n",
      "207 2.544792652130127\n",
      "208 2.4368510246276855\n",
      "209 2.3335494995117188\n",
      "210 2.2348220348358154\n",
      "211 2.1402554512023926\n",
      "212 2.0497829914093018\n",
      "213 1.96304452419281\n",
      "214 1.8800897598266602\n",
      "215 1.8009955883026123\n",
      "216 1.7249417304992676\n",
      "217 1.6523714065551758\n",
      "218 1.5828542709350586\n",
      "219 1.5162683725357056\n",
      "220 1.4525247812271118\n",
      "221 1.391539454460144\n",
      "222 1.3331401348114014\n",
      "223 1.2771257162094116\n",
      "224 1.2236528396606445\n",
      "225 1.172379493713379\n",
      "226 1.1233352422714233\n",
      "227 1.0763602256774902\n",
      "228 1.0314669609069824\n",
      "229 0.9882928729057312\n",
      "230 0.9471285343170166\n",
      "231 0.9076772928237915\n",
      "232 0.8697351217269897\n",
      "233 0.833534836769104\n",
      "234 0.7987523674964905\n",
      "235 0.7655495405197144\n",
      "236 0.733694851398468\n",
      "237 0.7031935453414917\n",
      "238 0.6740812063217163\n",
      "239 0.6461347341537476\n",
      "240 0.6192355155944824\n",
      "241 0.5936959385871887\n",
      "242 0.5689713954925537\n",
      "243 0.5454379916191101\n",
      "244 0.5228307843208313\n",
      "245 0.5013395547866821\n",
      "246 0.48059260845184326\n",
      "247 0.46075013279914856\n",
      "248 0.441676527261734\n",
      "249 0.42336615920066833\n",
      "250 0.4059646725654602\n",
      "251 0.38917529582977295\n",
      "252 0.37316828966140747\n",
      "253 0.3578198552131653\n",
      "254 0.34308549761772156\n",
      "255 0.3289361000061035\n",
      "256 0.3153854310512543\n",
      "257 0.30243343114852905\n",
      "258 0.29001256823539734\n",
      "259 0.278146356344223\n",
      "260 0.2666367292404175\n",
      "261 0.2557952105998993\n",
      "262 0.24527999758720398\n",
      "263 0.23520369827747345\n",
      "264 0.22556881606578827\n",
      "265 0.21634937822818756\n",
      "266 0.20749184489250183\n",
      "267 0.19900643825531006\n",
      "268 0.19087910652160645\n",
      "269 0.18307380378246307\n",
      "270 0.17563284933567047\n",
      "271 0.16845479607582092\n",
      "272 0.16156217455863953\n",
      "273 0.1549735963344574\n",
      "274 0.14864188432693481\n",
      "275 0.14258922636508942\n",
      "276 0.13672922551631927\n",
      "277 0.13118596374988556\n",
      "278 0.12582801282405853\n",
      "279 0.12070178240537643\n",
      "280 0.11579756438732147\n",
      "281 0.11108764261007309\n",
      "282 0.10657088458538055\n",
      "283 0.1022472232580185\n",
      "284 0.0980786383152008\n",
      "285 0.09409228712320328\n",
      "286 0.09029490500688553\n",
      "287 0.08662575483322144\n",
      "288 0.08310799300670624\n",
      "289 0.07972025126218796\n",
      "290 0.07652368396520615\n",
      "291 0.07342740148305893\n",
      "292 0.07044751197099686\n",
      "293 0.06760039180517197\n",
      "294 0.06485036760568619\n",
      "295 0.06222178414463997\n",
      "296 0.05970191955566406\n",
      "297 0.057290300726890564\n",
      "298 0.05498606711626053\n",
      "299 0.05277813971042633\n",
      "300 0.050633788108825684\n",
      "301 0.04858490824699402\n",
      "302 0.04663163796067238\n",
      "303 0.044760625809431076\n",
      "304 0.04294465854763985\n",
      "305 0.041232138872146606\n",
      "306 0.039583612233400345\n",
      "307 0.03798701986670494\n",
      "308 0.03646020218729973\n",
      "309 0.03499317169189453\n",
      "310 0.03360063582658768\n",
      "311 0.032252609729766846\n",
      "312 0.030962012708187103\n",
      "313 0.02971941977739334\n",
      "314 0.028535431250929832\n",
      "315 0.0273988489061594\n",
      "316 0.0263032466173172\n",
      "317 0.025249656289815903\n",
      "318 0.024250710383057594\n",
      "319 0.023268982768058777\n",
      "320 0.022349346429109573\n",
      "321 0.021453890949487686\n",
      "322 0.02060660347342491\n",
      "323 0.019784627482295036\n",
      "324 0.0189969502389431\n",
      "325 0.01825457252562046\n",
      "326 0.01752270571887493\n",
      "327 0.016835695132613182\n",
      "328 0.016160566359758377\n",
      "329 0.015527535229921341\n",
      "330 0.014918150380253792\n",
      "331 0.0143230389803648\n",
      "332 0.01376017089933157\n",
      "333 0.013222606852650642\n",
      "334 0.012706981040537357\n",
      "335 0.012214621528983116\n",
      "336 0.011728987097740173\n",
      "337 0.011270124465227127\n",
      "338 0.010834167711436749\n",
      "339 0.010411886498332024\n",
      "340 0.010008150711655617\n",
      "341 0.009615239687263966\n",
      "342 0.00924647320061922\n",
      "343 0.008878045715391636\n",
      "344 0.008541261777281761\n",
      "345 0.00821297150105238\n",
      "346 0.007902050390839577\n",
      "347 0.0075988443568348885\n",
      "348 0.007311799563467503\n",
      "349 0.007021285593509674\n",
      "350 0.006756495218724012\n",
      "351 0.00649796100333333\n",
      "352 0.0062509928829967976\n",
      "353 0.006014533340930939\n",
      "354 0.0057893842458724976\n",
      "355 0.00556781655177474\n",
      "356 0.0053619579412043095\n",
      "357 0.005159216467291117\n",
      "358 0.004965458996593952\n",
      "359 0.004778821021318436\n",
      "360 0.0046027242206037045\n",
      "361 0.004431408829987049\n",
      "362 0.004264991730451584\n",
      "363 0.004107569810003042\n",
      "364 0.0039575789123773575\n",
      "365 0.003811505390331149\n",
      "366 0.003672317834571004\n",
      "367 0.0035402136854827404\n",
      "368 0.003411912126466632\n",
      "369 0.003290037624537945\n",
      "370 0.003169843927025795\n",
      "371 0.0030525203328579664\n",
      "372 0.0029465099796652794\n",
      "373 0.002841605804860592\n",
      "374 0.0027412623167037964\n",
      "375 0.0026421714574098587\n",
      "376 0.002549441996961832\n",
      "377 0.002459346316754818\n",
      "378 0.0023718506563454866\n",
      "379 0.002292786492034793\n",
      "380 0.0022130324505269527\n",
      "381 0.002138560637831688\n",
      "382 0.0020624708849936724\n",
      "383 0.0019931418355554342\n",
      "384 0.0019289806950837374\n",
      "385 0.0018630761187523603\n",
      "386 0.001799722551368177\n",
      "387 0.001739691011607647\n",
      "388 0.0016844875644892454\n",
      "389 0.001626664656214416\n",
      "390 0.0015743037220090628\n",
      "391 0.0015224919188767672\n",
      "392 0.0014718861784785986\n",
      "393 0.0014241283060982823\n",
      "394 0.001382118440233171\n",
      "395 0.0013372218236327171\n",
      "396 0.001294218935072422\n",
      "397 0.0012522145407274365\n",
      "398 0.0012128748930990696\n",
      "399 0.0011762358481064439\n",
      "400 0.0011414826149120927\n",
      "401 0.0011058291420340538\n",
      "402 0.0010730773210525513\n",
      "403 0.0010415627621114254\n",
      "404 0.001011245884001255\n",
      "405 0.0009796733502298594\n",
      "406 0.0009512827382422984\n",
      "407 0.0009234538883902133\n",
      "408 0.0008953828364610672\n",
      "409 0.0008695409051142633\n",
      "410 0.0008424457046203315\n",
      "411 0.0008191467495635152\n",
      "412 0.0007966677658259869\n",
      "413 0.0007734157843515277\n",
      "414 0.0007517277263104916\n",
      "415 0.0007304389146156609\n",
      "416 0.0007094398024491966\n",
      "417 0.0006890076911076903\n",
      "418 0.0006708643049933016\n",
      "419 0.0006534223211929202\n",
      "420 0.0006354138604365289\n",
      "421 0.0006183471414260566\n",
      "422 0.0006011442164890468\n",
      "423 0.0005842194659635425\n",
      "424 0.0005695735453628004\n",
      "425 0.0005538005498237908\n",
      "426 0.0005381850642152131\n",
      "427 0.0005264410865493119\n",
      "428 0.0005119881243444979\n",
      "429 0.0004979990771971643\n",
      "430 0.0004865909868385643\n",
      "431 0.0004743351601064205\n",
      "432 0.00046108500100672245\n",
      "433 0.0004500045906752348\n",
      "434 0.00043896122951991856\n",
      "435 0.000428110477514565\n",
      "436 0.0004178660165052861\n",
      "437 0.00040841507143341005\n",
      "438 0.00039850728353485465\n",
      "439 0.00038943273830227554\n",
      "440 0.00037969090044498444\n",
      "441 0.00037079560570418835\n",
      "442 0.00036095312680117786\n",
      "443 0.000352369126630947\n",
      "444 0.00034497142769396305\n",
      "445 0.0003367891476955265\n",
      "446 0.00032852747244760394\n",
      "447 0.0003224373795092106\n",
      "448 0.00031506409868597984\n",
      "449 0.00030753330793231726\n",
      "450 0.0003004755999427289\n",
      "451 0.0002944570151157677\n",
      "452 0.0002875419449992478\n",
      "453 0.0002813212340697646\n",
      "454 0.00027551405946724117\n",
      "455 0.00026982263079844415\n",
      "456 0.00026376667665317655\n",
      "457 0.0002576389233581722\n",
      "458 0.00025238984380848706\n",
      "459 0.00024782269611023366\n",
      "460 0.0002430453896522522\n",
      "461 0.0002385252300882712\n",
      "462 0.00023343661450780928\n",
      "463 0.0002298764156876132\n",
      "464 0.00022452369739767164\n",
      "465 0.00021957056014798582\n",
      "466 0.000215415027923882\n",
      "467 0.000211363221751526\n",
      "468 0.0002068538888124749\n",
      "469 0.0002034411154454574\n",
      "470 0.00019957596668973565\n",
      "471 0.00019505710224620998\n",
      "472 0.00019195428467355669\n",
      "473 0.00018796537187881768\n",
      "474 0.00018464379536453635\n",
      "475 0.00018118569278158247\n",
      "476 0.00017756941088009626\n",
      "477 0.00017421969096176326\n",
      "478 0.00017127767205238342\n",
      "479 0.00016905264055822045\n",
      "480 0.0001662092108745128\n",
      "481 0.00016291449719574302\n",
      "482 0.00016055656305979937\n",
      "483 0.0001573422778164968\n",
      "484 0.00015536000137217343\n",
      "485 0.000151922446093522\n",
      "486 0.0001492554001742974\n",
      "487 0.00014744469081051648\n",
      "488 0.00014478244702331722\n",
      "489 0.0001424298679921776\n",
      "490 0.00013984243560116738\n",
      "491 0.00013704758021049201\n",
      "492 0.00013468167162500322\n",
      "493 0.0001332108222413808\n",
      "494 0.00013061516801826656\n",
      "495 0.00012849780614487827\n",
      "496 0.00012656388571485877\n",
      "497 0.00012486209743656218\n",
      "498 0.00012266456906218082\n",
      "499 0.00012109320960007608\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
